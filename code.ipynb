{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4CaMER2uL4wh",
    "outputId": "52dd229f-940d-4799-d749-8959e4a66d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "pip install torch pandas numpy scikit-learn tqdm transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLpVIFoGNLW0",
    "outputId": "48531167-47c0-4ccb-9f2b-f204734fadf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yo01nHovL2VP",
    "outputId": "75aaa196-4257-4c2e-dd5c-c2544e46ea4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▌         | 78/1265 [00:36<09:07,  2.17it/s, loss=11.3]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  =======  =======\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3\n",
    "MASK_PROB = 0.15\n",
    "EMB_DIM = 512\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#  =======  =======\n",
    "drive_path = \"/content/drive/MyDrive/RS\"\n",
    "train_csv = os.path.join(drive_path, \"train.csv\")\n",
    "sample_csv = os.path.join(drive_path, \"sample_submission.csv\")\n",
    "meta_csv = os.path.join(drive_path, \"item_meta.csv\")\n",
    "model_path = os.path.join(drive_path, \"bert4rec.pt\")\n",
    "output_csv = os.path.join(drive_path, \"submission.csv\")\n",
    "\n",
    "#  =======  =======\n",
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, user_sequences, max_len=50, mask_prob=0.2, num_items=10000):\n",
    "        self.user_sequences = user_sequences\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.user_sequences[idx][-self.max_len:]\n",
    "        pad_len = self.max_len - len(seq)\n",
    "        seq = [0] * pad_len + seq\n",
    "\n",
    "        tokens = []\n",
    "        labels = []\n",
    "\n",
    "        for item in seq:\n",
    "            if item == 0:\n",
    "                tokens.append(0)\n",
    "                labels.append(0)\n",
    "            elif np.random.rand() < self.mask_prob:\n",
    "                tokens.append(self.num_items + 1)  #  MASK token\n",
    "                labels.append(item)\n",
    "            else:\n",
    "                tokens.append(item)\n",
    "                labels.append(0)\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.LongTensor(labels)\n",
    "\n",
    "#  ======= BERT4Rec  =======\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, num_items, d_model=128, max_len=50, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items + 2, d_model, padding_idx=0)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_model * 4, dropout, batch_first=True)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, num_items + 2)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        pos = torch.arange(self.max_len, device=input_seq.device).unsqueeze(0).expand_as(input_seq)\n",
    "        x = self.item_embedding(input_seq) + self.position_embedding(pos)\n",
    "        x = self.transformer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "#  ======= NCF =======\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=64):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users + 1, emb_dim)\n",
    "        self.item_emb = nn.Embedding(num_items + 1, emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        x = torch.cat([u, i], dim=1)\n",
    "        return self.mlp(x).squeeze()\n",
    "\n",
    "#  =======  =======\n",
    "def get_user_sequences(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    user_seq = df.groupby('user_id')['item_id'].apply(list)\n",
    "    return user_seq.tolist(), df['item_id'].max()\n",
    "\n",
    "#  ======= Item Popularity  =======\n",
    "def get_item_popularity(train_df):\n",
    "    return dict(train_df['item_id'].value_counts(normalize=True))\n",
    "\n",
    "#  ======= Reciprocal Rank Fusion =======\n",
    "def rrf_fusion(rank_lists, k=60):\n",
    "    scores = Counter()\n",
    "    for rank in rank_lists:\n",
    "        for i, item in enumerate(rank):\n",
    "            scores[item] += 1 / (k + i)\n",
    "    ranked = [x[0] for x in sorted(scores.items(), key=lambda x: -x[1])]\n",
    "    return ranked\n",
    "\n",
    "#  ======= Title embedding  =======\n",
    "def load_title_embeddings(meta_csv, model_name=\"distilbert-base-uncased\", device=\"cpu\"):\n",
    "    if not os.path.exists(meta_csv):\n",
    "        return {}\n",
    "\n",
    "    meta_df = pd.read_csv(meta_csv)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    title_embeddings = {}\n",
    "    for _, row in tqdm(meta_df.iterrows(), total=len(meta_df), desc=\" Edit title\"):\n",
    "        item_id = row[\"item_id\"]\n",
    "        title = str(row[\"title\"])\n",
    "        inputs = tokenizer(title, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).last_hidden_state[:, 0, :]  #  [CLS] token\n",
    "            title_embeddings[item_id] = output.squeeze().cpu()\n",
    "\n",
    "    return title_embeddings\n",
    "\n",
    "#  ======= =======\n",
    "def inference_and_fusion():\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    sample_df = pd.read_csv(sample_csv)\n",
    "    user_sequences = train_df.groupby(\"user_id\")[\"item_id\"].apply(list).to_dict()\n",
    "    num_items = train_df[\"item_id\"].max()\n",
    "    num_users = train_df[\"user_id\"].max()\n",
    "\n",
    "    model = BERT4Rec(num_items=num_items, d_model=EMB_DIM, max_len=MAX_LEN, num_layers=3).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    title_embeddings = load_title_embeddings(meta_csv, device=DEVICE)\n",
    "    all_items = list(title_embeddings.keys())\n",
    "    all_embs = torch.stack([title_embeddings[i] for i in all_items]).to(DEVICE)\n",
    "\n",
    "    popularity = get_item_popularity(train_df)\n",
    "\n",
    "    submission = []\n",
    "    print(\" Fusion recommendations are being generated...\")\n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        user_id = row[\"user_id\"]\n",
    "        seq = user_sequences.get(user_id, [])[-MAX_LEN:]\n",
    "        seq = [0] * (MAX_LEN - len(seq)) + seq\n",
    "\n",
    "        masked_seq = []\n",
    "        for item in seq:\n",
    "            if item == 0:\n",
    "                masked_seq.append(0)\n",
    "            elif np.random.rand() < MASK_PROB:\n",
    "                masked_seq.append(num_items + 1)\n",
    "            else:\n",
    "                masked_seq.append(item)\n",
    "\n",
    "        input_seq = torch.LongTensor(masked_seq).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)[0]  #  shape: [seq_len, num_items+2]\n",
    "            mask_indices = [i for i, t in enumerate(masked_seq) if t == num_items + 1]\n",
    "\n",
    "            score_pool = []\n",
    "            for i in mask_indices:\n",
    "                score = logits[i]\n",
    "                top_items = torch.topk(score, 100).indices.cpu().numpy().tolist()\n",
    "                score_pool.extend(top_items)\n",
    "\n",
    "            topk_bert = [x for x, _ in Counter(score_pool).most_common(100) if x >= 2]\n",
    "\n",
    "        pop_sorted = [i for i, _ in sorted(popularity.items(), key=lambda x: -x[1])]\n",
    "        topk_pop = pop_sorted[:100]\n",
    "\n",
    "        if len(seq) > 1:\n",
    "            last_item = seq[-2]\n",
    "            if last_item in title_embeddings:\n",
    "                query_emb = title_embeddings[last_item].to(DEVICE)\n",
    "                sim = F.cosine_similarity(query_emb.unsqueeze(0), all_embs)\n",
    "                topk_idx = sim.topk(100).indices.cpu().numpy()\n",
    "                topk_title = [all_items[i] for i in topk_idx if all_items[i] >= 2]\n",
    "            else:\n",
    "                topk_title = []\n",
    "        else:\n",
    "            topk_title = []\n",
    "\n",
    "        fused = rrf_fusion([topk_bert, topk_pop, topk_title])[:10]\n",
    "\n",
    "        submission.append({\n",
    "            \"ID\": row[\"ID\"],\n",
    "            \"user_id\": user_id,\n",
    "            \"item_id\": \",\".join(map(str, fused))\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(submission).to_csv(output_csv, index=False)\n",
    "    print(f\" Submissions have been saved to the：{output_csv}\")\n",
    "\n",
    "#  =======  =======\n",
    "def train():\n",
    "    sequences, max_item_id = get_user_sequences(train_csv)\n",
    "    dataset = BERT4RecDataset(sequences, max_len=MAX_LEN, mask_prob=MASK_PROB, num_items=max_item_id)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = BERT4Rec(num_items=max_item_id, d_model=EMB_DIM, max_len=MAX_LEN, num_layers=3).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "        for tokens, labels in loop:\n",
    "            tokens, labels = tokens.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(tokens)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"[Epoch {epoch}] Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\" The model has been saved to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    inference_and_fusion()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
